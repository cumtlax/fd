"""flower-tutorial: A Flower / PyTorch app."""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split, Subset
import torchvision.datasets as datasets
# Flower æä¾›çš„è”é‚¦æ•°æ®é›†åŠ è½½å·¥å…·
from flwr_datasets import FederatedDataset
# å°†æ•°æ®åˆ’åˆ†ä¸ºå¤šä¸ªâ€œç‹¬ç«‹åŒåˆ†å¸ƒâ€çš„å®¢æˆ·ç«¯åˆ†åŒº
from flwr_datasets.partitioner import IidPartitioner
from torch.utils.data import DataLoader
# å›¾åƒé¢„å¤„ç†å·¥å…·
from torchvision.transforms import Compose, Normalize, ToTensor


class Net(nn.Module):
    """Model (simple CNN adapted from 'PyTorch: A 60 Minute Blitz')"""

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

# ç”¨äºç¼“å­˜ FederatedDataset å¯¹è±¡ï¼Œé¿å…æ¯ä¸ªå®¢æˆ·ç«¯é‡å¤åˆå§‹åŒ–ã€‚
fds = None  # Cache FederatedDataset

# å›¾åƒé¢„å¤„ç†æµç¨‹
# ToTensor()ï¼šå°† PIL å›¾åƒè½¬ä¸º Tensorï¼Œå¹¶å½’ä¸€åŒ–åˆ° [0,1]
# Normalize(mean, std)ï¼šæ ‡å‡†åŒ–ä¸º x = (x - mean) / std
# è¿™é‡Œç”¨ (0.5, 0.5, 0.5) æ˜¯ä¸ºäº†æŠŠ [0,1] æ˜ å°„åˆ° [-1,1]
pytorch_transforms = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])



import torchvision.transforms as transforms

# å›¾åƒé¢„å¤„ç†ï¼ˆå’ŒåŸæ¥ä¸€è‡´ï¼‰
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
])


def apply_transforms(batch):
    """Apply transforms to the partition from FederatedDataset."""
    batch["img"] = [pytorch_transforms(img) for img in batch["img"]]
    return batch



def load_data(partition_id: int, num_partitions: int):
    """
    Load CIFAR-10 data from local disk and partition it into `num_partitions`.
    Each client gets a non-overlapping subset.
    """
    # Step 1: Download or load CIFAR-10 locally
    trainset = datasets.CIFAR10(root="./data", train=True, download=True, transform=transform_train)
    testset = datasets.CIFAR10(root="./data", train=False, download=True, transform=transform_test)

    # Step 2: Split training set into `num_partitions` equal parts (non-IID æ¨¡å¼å¯åç»­æ‰©å±•)
    total_size = len(trainset)
    partition_size = total_size // num_partitions
    lengths = [partition_size] * num_partitions
    # å¤„ç†ä¸èƒ½æ•´é™¤çš„æƒ…å†µ
    lengths[-1] += total_size - sum(lengths)

    # éšæœºåˆ’åˆ†æ•°æ®ï¼ˆæ¨¡æ‹Ÿ IID åˆ†å¸ƒï¼‰
    partitions = random_split(trainset, lengths, generator=torch.Generator().manual_seed(42))

    # Step 3: Select the partition for this client
    train_partition = partitions[partition_id]

    # Step 4: Use same test set for all clients (or optionally also split)
    # å¦‚æœä½ ä¹Ÿæƒ³è®©æ¯ä¸ªå®¢æˆ·ç«¯æœ‰è‡ªå·±çš„æµ‹è¯•é›†ï¼Œå¯ä»¥ç±»ä¼¼åˆ’åˆ† testset
    test_partition = testset

    # Step 5: Create DataLoaders
    trainloader = DataLoader(train_partition, batch_size=32, shuffle=True)
    testloader = DataLoader(test_partition, batch_size=32, shuffle=False)  # å…¨å±€æµ‹è¯•é›†ä¸æ‰“ä¹±

    return trainloader, testloader


def train(net, trainloader, epochs, lr, device):
    """Train the model on the training set."""
    # æŠŠæ¨¡å‹æ”¾åˆ°æŒ‡å®šè®¾å¤‡
    # .to(device) çš„ä½œç”¨æ˜¯ï¼š
    # å¦‚æœæœ‰ GPUï¼Œå°±æŠŠæ¨¡å‹å‚æ•°ä» CPU æ‹·è´åˆ° GPU æ˜¾å­˜ä¸­
    # è¿™æ ·åç»­è®¡ç®—ï¼ˆå·ç§¯ã€çŸ©é˜µä¹˜ç­‰ï¼‰ä¼šç”¨ GPU åŠ é€Ÿï¼Œå¿«å¾ˆå¤šï¼
    net.to(device)  # move model to GPU if available
    # å®šä¹‰æŸå¤±å‡½æ•°
    criterion = torch.nn.CrossEntropyLoss().to(device)
    # å®šä¹‰ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    # å‘Šè¯‰æ¨¡å‹ï¼šâ€œæˆ‘ç°åœ¨è¦å¼€å§‹è®­ç»ƒäº†ï¼â€
    # å½±å“æŸäº›å±‚çš„è¡Œä¸ºï¼Œæ¯”å¦‚ï¼š
    # Dropout å±‚ï¼šè®­ç»ƒæ—¶éšæœºä¸¢å¼ƒç¥ç»å…ƒï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
    # BatchNorm å±‚ï¼šä½¿ç”¨å½“å‰ batch çš„å‡å€¼å’Œæ–¹å·®
    # å¦‚æœå¿˜è®°å†™è¿™å¥ï¼ŒDropout å¯èƒ½ä¸ç”Ÿæ•ˆï¼Œå¯¼è‡´è®­ç»ƒå’Œæµ‹è¯•è¡Œä¸ºä¸ä¸€è‡´ã€‚
    net.train()
    # ç”¨äºç´¯è®¡æ¯ä¸ª batch çš„æŸå¤±å€¼
    # æœ€åé™¤ä»¥ batch æ•°é‡ï¼Œå¾—åˆ°å¹³å‡æŸå¤±ï¼ˆç”¨äºè¿”å›ï¼‰
    running_loss = 0.0
    for _ in range(epochs):
        # trainloader æ˜¯å‰é¢ DataLoader åˆ›å»ºçš„è¿­ä»£å™¨
        # æ¯æ¬¡è¿”å›ä¸€ä¸ª batchï¼Œæ ¼å¼å¦‚ï¼š
        # python
        # ç¼–è¾‘
        # {
        #   "img": [img1, img2, ..., img32],  # 32å¼ å›¾åƒ
        #   "label": [3, 8, 0, ..., 5]        # 32ä¸ªæ ‡ç­¾
        # }
        for images, labels in trainloader:
            # æŠŠå›¾åƒå’Œæ ‡ç­¾ä» CPU ç§»åˆ° GPUï¼ˆå¦‚æœ device='cuda'ï¼‰
            # å¿…é¡»å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼Œå¦åˆ™ä¼šæŠ¥é”™ï¼
            # ğŸ“Œ æ³¨æ„ï¼šbatch["img"] æ˜¯ä¸€ä¸ª Tensorï¼Œå½¢çŠ¶ä¸º (32, 3, 32, 32)
            images = images.to(device)
            labels = labels.to(device)
            optimizer.zero_grad()
            loss = criterion(net(images), labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
    avg_trainloss = running_loss / len(trainloader)
    return avg_trainloss


def test(net, testloader, device):
    """Validate the model on the test set."""
    net.to(device)
    criterion = torch.nn.CrossEntropyLoss()
    correct, loss = 0, 0.0
    with torch.no_grad():
        for images, labels in testloader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = net(images)
            loss += criterion(outputs, labels).item()
            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()
    accuracy = correct / len(testloader.dataset)
    loss = loss / len(testloader)
    return loss, accuracy




# partition_id: æˆ‘æ˜¯ç¬¬å‡ ä¸ªå®¢æˆ·ç«¯
# num_partitions: æŠŠæ•´ä¸ªæ•°æ®é›†å¹³å‡åˆ†æˆå¤šå°‘ä»½ï¼Œæ¯ä¸€ä»½ç»™ä¸€ä¸ªâ€œå®¢æˆ·ç«¯â€ï¼ˆclientï¼‰ä½¿ç”¨ã€‚
# num_partitions = 10 â†’ æŠŠæ•°æ®åˆ†æˆ 10 ä»½ï¼Œå‘ç»™ 10 ä¸ªå®¢æˆ·ç«¯
# num_partitions = 100 â†’ åˆ†æˆ 100 ä»½ï¼Œå‘ç»™ 100 ä¸ªå®¢æˆ·ç«¯
# def load_data(partition_id: int, num_partitions: int):
#     """Load partition CIFAR10 data."""
#     # Only initialize `FederatedDataset` once
#
#     # ä½¿ç”¨äº†å…¨å±€å˜é‡ fds æ¥å­˜å‚¨ä¸€ä¸ª FederatedDataset å®ä¾‹ã€‚è¿™æ˜¯ä¸ºäº†é¿å…å¤šæ¬¡åˆå§‹åŒ– FederatedDataset å¯¹è±¡ï¼Œ
#     # ä»è€ŒèŠ‚çœèµ„æºå¹¶ä¿è¯æ•°æ®çš„ä¸€è‡´æ€§ã€‚å¦‚æœ fds ä¸å­˜åœ¨ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ–ã€‚
#     global fds
#     if fds is None:
#         # IID = Independent and Identically Distributed
#         # Independent	ç‹¬ç«‹	æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®æ˜¯ç‹¬ç«‹çš„ï¼Œäº’ä¸å½±å“
#         # Identically Distributed	åŒåˆ†å¸ƒ	æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®åˆ†å¸ƒç›¸ä¼¼ï¼ˆæ¯”å¦‚éƒ½æœ‰çŒ«ã€ç‹—ã€è½¦ç­‰ï¼‰
#         #  ä¸¾ä¸ªä¾‹å­ï¼ˆCIFAR-10 æ•°æ®é›†ï¼‰ï¼š
#         #
#         # æ€»å…± 10 ç±»ï¼šé£æœºã€æ±½è½¦ã€é¸Ÿã€çŒ«ã€é¹¿ã€ç‹—ã€é’è›™ã€é©¬ã€èˆ¹ã€å¡è½¦
#         # å¦‚æœæ˜¯ IID åˆ†å‰²ï¼š
#         # å®¢æˆ·ç«¯ 0ï¼šæœ‰ 10% é£æœºã€10% çŒ«ã€10% è½¦â€¦â€¦
#         # å®¢æˆ·ç«¯ 1ï¼šä¹Ÿæœ‰ 10% é£æœºã€10% çŒ«ã€10% è½¦â€¦â€¦
#         # â€¦â€¦
#         # ğŸ‘‰ æ‰€æœ‰å®¢æˆ·ç«¯çœ‹åˆ°çš„â€œä¸–ç•Œâ€æ˜¯ç›¸ä¼¼çš„ã€‚
#         # IidPartitioner æ˜¯ä¸€ä¸ªâ€œå…¬å¹³åˆ†æ•°æ®â€çš„å·¥å…·ï¼Œå®ƒæŠŠæ•´ä¸ªæ•°æ®é›†å¹³å‡ã€éšæœºåœ°åˆ†æˆ num_partitions ä»½ï¼Œç¡®ä¿æ¯ä¸€ä»½çš„æ•°æ®åˆ†å¸ƒéƒ½å·®ä¸å¤š
#         partitioner = IidPartitioner(num_partitions=num_partitions)
#         # åˆå§‹åŒ– FederatedDatasetï¼ŒæŒ‡å®šäº†æ•°æ®é›†åç§° "uoft-cs/cifar10" å’Œè®­ç»ƒé›†çš„åˆ†åŒºç­–ç•¥ã€‚
#         fds = FederatedDataset(
#             dataset="uoft-cs/cifar10",
#             partitioners={"train": partitioner},
#         )
#     # æ ¹æ®ä¼ å…¥çš„ partition_id åŠ è½½å¯¹åº”çš„æ•°æ®åˆ†åŒºã€‚è¿™æ„å‘³ç€æ¯ä¸ªèŠ‚ç‚¹åªä¼šçœ‹åˆ°æ•´ä¸ªæ•°æ®é›†çš„ä¸€ä¸ªå­é›†ã€‚
#     partition = fds.load_partition(partition_id)
#     # Divide data on each node: 80% train, 20% test
#     partition_train_test = partition.train_test_split(test_size=0.2, seed=42)
#     # Construct dataloaders
#     partition_train_test = partition_train_test.with_transform(apply_transforms)
#     trainloader = DataLoader(partition_train_test["train"], batch_size=32, shuffle=True)
#     testloader = DataLoader(partition_train_test["test"], batch_size=32)
#     return trainloader, testloader