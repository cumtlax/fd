"""flower-tutorial: A Flower / PyTorch app."""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
from torch.utils.data import random_split
from torchvision.transforms import Compose, Normalize, ToTensor
import torchvision.transforms as transforms

class Net(nn.Module):
    """Model (simple CNN adapted from 'PyTorch: A 60 Minute Blitz')"""

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

# ç”¨äºç¼“å­˜ FederatedDataset å¯¹è±¡ï¼Œé¿å…æ¯ä¸ªå®¢æˆ·ç«¯é‡å¤åˆå§‹åŒ–ã€‚
fds = None  # Cache FederatedDataset

# å›¾åƒé¢„å¤„ç†æµç¨‹
# ToTensor()ï¼šå°† PIL å›¾åƒè½¬ä¸º Tensorï¼Œå¹¶å½’ä¸€åŒ–åˆ° [0,1]
# Normalize(mean, std)ï¼šæ ‡å‡†åŒ–ä¸º x = (x - mean) / std
# è¿™é‡Œç”¨ (0.5, 0.5, 0.5) æ˜¯ä¸ºäº†æŠŠ [0,1] æ˜ å°„åˆ° [-1,1]
pytorch_transforms = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

# å›¾åƒé¢„å¤„ç†ï¼ˆå’ŒåŸæ¥ä¸€è‡´ï¼‰
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
])

def load_data(partition_id: int, num_partitions: int):
    """
    åŠ è½½å¹¶åˆ’åˆ†CIFAR-10æ•°æ®é›†ï¼Œä¸ºè”é‚¦å­¦ä¹ ä¸­çš„æŸä¸ªå®¢æˆ·ç«¯è¿”å›å…¶æœ¬åœ°çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨ã€‚

    å‚æ•°:
        partition_id (int): å½“å‰å®¢æˆ·ç«¯çš„IDï¼ˆä»0å¼€å§‹ï¼‰ã€‚
        num_partitions (int): å°†è®­ç»ƒé›†åˆ’åˆ†ä¸ºå¤šå°‘ä¸ªéƒ¨åˆ†ï¼ˆå³å®¢æˆ·ç«¯æ€»æ•°ï¼‰ã€‚

    è¿”å›:
        tuple: åŒ…å«è®­ç»ƒæ•°æ®åŠ è½½å™¨(trainloader)å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨(testloader)çš„å…ƒç»„ã€‚
    """

    # 1. ä¸‹è½½å¹¶åŠ è½½å®Œæ•´çš„CIFAR-10è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # `root="./data"` æŒ‡å®šæ•°æ®å­˜å‚¨ç›®å½•
    # `train=True` è¡¨ç¤ºåŠ è½½è®­ç»ƒé›†
    # `download=True` å¦‚æœæœ¬åœ°æ²¡æœ‰æ•°æ®ï¼Œåˆ™è‡ªåŠ¨ä¸‹è½½
    # `transform=transform_train` åº”ç”¨é¢„å®šä¹‰çš„æ•°æ®å¢å¼ºå’Œå½’ä¸€åŒ–å˜æ¢
    trainset = datasets.CIFAR10(root="./data", train=True, download=True, transform=transform_train)
    testset = datasets.CIFAR10(root="./data", train=False, download=True, transform=transform_test)

    # 2. è®¡ç®—æ¯ä¸ªæ•°æ®åˆ†åŒºï¼ˆå®¢æˆ·ç«¯ï¼‰åº”åˆ†é…åˆ°çš„æ ·æœ¬æ•°é‡
    total_size = len(trainset)  # è·å–è®­ç»ƒé›†æ€»æ ·æœ¬æ•° (50000 for CIFAR-10)
    partition_size = total_size // num_partitions  # è®¡ç®—æ¯ä¸ªåˆ†åŒºçš„åŸºç¡€å¤§å°
    lengths = [partition_size] * num_partitions  # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸€ä¸ªåˆ†åŒºçš„å¤§å°

    # 3. å¤„ç†æ— æ³•æ•´é™¤çš„æƒ…å†µï¼šå°†ä½™ä¸‹çš„æ ·æœ¬åˆ†é…ç»™æœ€åä¸€ä¸ªåˆ†åŒº
    # ä¾‹å¦‚ï¼Œ50000ä¸ªæ ·æœ¬åˆ†ç»™3ä¸ªå®¢æˆ·ç«¯ï¼Œå‰ä¸¤ä¸ªå„16666ï¼Œæœ€åä¸€ä¸ª16668
    lengths[-1] += total_size - sum(lengths)

    # 4. å°†è®­ç»ƒé›†éšæœºåˆ’åˆ†ä¸ºnum_partitionsä¸ªä¸ç›¸äº¤çš„å­é›†
    # ä½¿ç”¨`random_split`å‡½æ•°è¿›è¡Œåˆ’åˆ†
    # `generator=torch.Generator().manual_seed(42)` ç¡®ä¿åˆ’åˆ†ç»“æœå¯å¤ç°ï¼ˆç›¸åŒçš„éšæœºç§å­ï¼‰
    partitions = random_split(trainset, lengths, generator=torch.Generator().manual_seed(42))

    # 5. ä¸ºå½“å‰å®¢æˆ·ç«¯ï¼ˆç”±partition_idæŒ‡å®šï¼‰é€‰æ‹©å¯¹åº”çš„è®­ç»ƒæ•°æ®åˆ†åŒº
    train_partition = partitions[partition_id]

    # 6. æµ‹è¯•é›†ä¸åˆ†å‰²ï¼Œæ‰€æœ‰å®¢æˆ·ç«¯å…±äº«åŒä¸€ä¸ªå…¨å±€æµ‹è¯•é›†
    # è¿™æ ·å¯ä»¥ç»Ÿä¸€è¯„ä¼°æ¨¡å‹åœ¨ç›¸åŒæµ‹è¯•æ•°æ®ä¸Šçš„æ€§èƒ½
    test_partition = testset

    # 7. åˆ›å»ºæ•°æ®åŠ è½½å™¨(DataLoader)ï¼Œç”¨äºæ‰¹é‡è¯»å–æ•°æ®
    # è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼šä½¿ç”¨å®¢æˆ·ç«¯çš„è®­ç»ƒåˆ†åŒºï¼Œbatch_size=32ï¼Œå¹¶æ‰“ä¹±é¡ºåº(shuffle=True)ä»¥æé«˜è®­ç»ƒæ•ˆæœ
    trainloader = DataLoader(train_partition, batch_size=32, shuffle=True)

    # æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼šä½¿ç”¨å®Œæ•´çš„æµ‹è¯•é›†ï¼Œbatch_size=32ï¼Œä¸æ‰“ä¹±é¡ºåº(shuffle=False)
    # é€šå¸¸æµ‹è¯•æ—¶ä¸éœ€æ‰“ä¹±ï¼Œä¸”ä¿æŒä¸€è‡´çš„é¡ºåºæœ‰åŠ©äºç»“æœæ¯”è¾ƒ
    testloader = DataLoader(test_partition, batch_size=32, shuffle=False)

    # 8. è¿”å›è¯¥å®¢æˆ·ç«¯çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
    return trainloader, testloader

def train(net, trainloader, epochs, lr, device):
    """Train the model on the training set."""
    # æŠŠæ¨¡å‹æ”¾åˆ°æŒ‡å®šè®¾å¤‡
    # .to(device) çš„ä½œç”¨æ˜¯ï¼š
    # å¦‚æœæœ‰ GPUï¼Œå°±æŠŠæ¨¡å‹å‚æ•°ä» CPU æ‹·è´åˆ° GPU æ˜¾å­˜ä¸­
    # è¿™æ ·åç»­è®¡ç®—ï¼ˆå·ç§¯ã€çŸ©é˜µä¹˜ç­‰ï¼‰ä¼šç”¨ GPU åŠ é€Ÿï¼Œå¿«å¾ˆå¤šï¼
    net.to(device)  # move model to GPU if available
    # å®šä¹‰æŸå¤±å‡½æ•°
    criterion = torch.nn.CrossEntropyLoss().to(device)
    # å®šä¹‰ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    # å‘Šè¯‰æ¨¡å‹ï¼šâ€œæˆ‘ç°åœ¨è¦å¼€å§‹è®­ç»ƒäº†ï¼â€
    # å½±å“æŸäº›å±‚çš„è¡Œä¸ºï¼Œæ¯”å¦‚ï¼š
    # Dropout å±‚ï¼šè®­ç»ƒæ—¶éšæœºä¸¢å¼ƒç¥ç»å…ƒï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
    # BatchNorm å±‚ï¼šä½¿ç”¨å½“å‰ batch çš„å‡å€¼å’Œæ–¹å·®
    # å¦‚æœå¿˜è®°å†™è¿™å¥ï¼ŒDropout å¯èƒ½ä¸ç”Ÿæ•ˆï¼Œå¯¼è‡´è®­ç»ƒå’Œæµ‹è¯•è¡Œä¸ºä¸ä¸€è‡´ã€‚
    net.train()
    # ç”¨äºç´¯è®¡æ¯ä¸ª batch çš„æŸå¤±å€¼
    # æœ€åé™¤ä»¥ batch æ•°é‡ï¼Œå¾—åˆ°å¹³å‡æŸå¤±ï¼ˆç”¨äºè¿”å›ï¼‰
    running_loss = 0.0
    for _ in range(epochs):
        # trainloader æ˜¯å‰é¢ DataLoader åˆ›å»ºçš„è¿­ä»£å™¨
        # æ¯æ¬¡è¿”å›ä¸€ä¸ª batchï¼Œæ ¼å¼å¦‚ï¼š
        # python
        # ç¼–è¾‘
        # {
        #   "img": [img1, img2, ..., img32],  # 32å¼ å›¾åƒ
        #   "label": [3, 8, 0, ..., 5]        # 32ä¸ªæ ‡ç­¾
        # }
        for images, labels in trainloader:
            # æŠŠå›¾åƒå’Œæ ‡ç­¾ä» CPU ç§»åˆ° GPUï¼ˆå¦‚æœ device='cuda'ï¼‰
            # å¿…é¡»å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼Œå¦åˆ™ä¼šæŠ¥é”™ï¼
            # ğŸ“Œ æ³¨æ„ï¼šbatch["img"] æ˜¯ä¸€ä¸ª Tensorï¼Œå½¢çŠ¶ä¸º (32, 3, 32, 32)
            images = images.to(device)
            labels = labels.to(device)
            optimizer.zero_grad()
            loss = criterion(net(images), labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
    avg_trainloss = running_loss / len(trainloader)
    return avg_trainloss


def test(net, testloader, device):
    """Validate the model on the test set."""
    net.to(device)
    criterion = torch.nn.CrossEntropyLoss()
    correct, loss = 0, 0.0
    with torch.no_grad():
        for images, labels in testloader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = net(images)
            loss += criterion(outputs, labels).item()
            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()
    accuracy = correct / len(testloader.dataset)
    loss = loss / len(testloader)
    return loss, accuracy





# def load_data(partition_id: int, num_partitions: int):
#     """Load partition CIFAR10 data."""
#     # Only initialize `FederatedDataset` once
#
#     # ä½¿ç”¨äº†å…¨å±€å˜é‡ fds æ¥å­˜å‚¨ä¸€ä¸ª FederatedDataset å®ä¾‹ã€‚è¿™æ˜¯ä¸ºäº†é¿å…å¤šæ¬¡åˆå§‹åŒ– FederatedDataset å¯¹è±¡ï¼Œ
#     # ä»è€ŒèŠ‚çœèµ„æºå¹¶ä¿è¯æ•°æ®çš„ä¸€è‡´æ€§ã€‚å¦‚æœ fds ä¸å­˜åœ¨ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ–ã€‚
#     global fds
#     if fds is None:
#         # IID = Independent and Identically Distributed
#         # Independent	ç‹¬ç«‹	æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®æ˜¯ç‹¬ç«‹çš„ï¼Œäº’ä¸å½±å“
#         # Identically Distributed	åŒåˆ†å¸ƒ	æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®åˆ†å¸ƒç›¸ä¼¼ï¼ˆæ¯”å¦‚éƒ½æœ‰çŒ«ã€ç‹—ã€è½¦ç­‰ï¼‰
#         #  ä¸¾ä¸ªä¾‹å­ï¼ˆCIFAR-10 æ•°æ®é›†ï¼‰ï¼š
#         #
#         # æ€»å…± 10 ç±»ï¼šé£æœºã€æ±½è½¦ã€é¸Ÿã€çŒ«ã€é¹¿ã€ç‹—ã€é’è›™ã€é©¬ã€èˆ¹ã€å¡è½¦
#         # å¦‚æœæ˜¯ IID åˆ†å‰²ï¼š
#         # å®¢æˆ·ç«¯ 0ï¼šæœ‰ 10% é£æœºã€10% çŒ«ã€10% è½¦â€¦â€¦
#         # å®¢æˆ·ç«¯ 1ï¼šä¹Ÿæœ‰ 10% é£æœºã€10% çŒ«ã€10% è½¦â€¦â€¦
#         # â€¦â€¦
#         # ğŸ‘‰ æ‰€æœ‰å®¢æˆ·ç«¯çœ‹åˆ°çš„â€œä¸–ç•Œâ€æ˜¯ç›¸ä¼¼çš„ã€‚
#         # IidPartitioner æ˜¯ä¸€ä¸ªâ€œå…¬å¹³åˆ†æ•°æ®â€çš„å·¥å…·ï¼Œå®ƒæŠŠæ•´ä¸ªæ•°æ®é›†å¹³å‡ã€éšæœºåœ°åˆ†æˆ num_partitions ä»½ï¼Œç¡®ä¿æ¯ä¸€ä»½çš„æ•°æ®åˆ†å¸ƒéƒ½å·®ä¸å¤š
#         partitioner = IidPartitioner(num_partitions=num_partitions)
#         # åˆå§‹åŒ– FederatedDatasetï¼ŒæŒ‡å®šäº†æ•°æ®é›†åç§° "uoft-cs/cifar10" å’Œè®­ç»ƒé›†çš„åˆ†åŒºç­–ç•¥ã€‚
#         fds = FederatedDataset(
#             dataset="uoft-cs/cifar10",
#             partitioners={"train": partitioner},
#         )
#     # æ ¹æ®ä¼ å…¥çš„ partition_id åŠ è½½å¯¹åº”çš„æ•°æ®åˆ†åŒºã€‚è¿™æ„å‘³ç€æ¯ä¸ªèŠ‚ç‚¹åªä¼šçœ‹åˆ°æ•´ä¸ªæ•°æ®é›†çš„ä¸€ä¸ªå­é›†ã€‚
#     partition = fds.load_partition(partition_id)
#     # Divide data on each node: 80% train, 20% test
#     partition_train_test = partition.train_test_split(test_size=0.2, seed=42)
#     # Construct dataloaders
#     partition_train_test = partition_train_test.with_transform(apply_transforms)
#     trainloader = DataLoader(partition_train_test["train"], batch_size=32, shuffle=True)
#     testloader = DataLoader(partition_train_test["test"], batch_size=32)
#     return trainloader, testloader